---
title: "How I built a Node.js Service to Clone my AWS S3 Buckets"
---

![](https://cdn-images-1.medium.com/max/800/1*7pNMMZbTGFovZSlL9b10ew.jpeg)_Photo by [Malte Wingen](https://medium.com/r/?url=https%3A%2F%2Funsplash.com%2Fphotos%2F4AG-k7KeNOg%3Futm_source%3Dunsplash%26utm_medium%3Dreferral%26utm_content%3DcreditCopyText) on [Unsplash](https://medium.com/r/?url=https%3A%2F%2Funsplash.com%2Fsearch%2Fphotos%2Fnode-js%3Futm_source%3Dunsplash%26utm_medium%3Dreferral%26utm_content%3DcreditCopyText)_

## Introduction

> **_This project is basically building a Node.js application to clone your AWS S3 buckets locally and recursively._**

_TBH, this project is a one-liner using the AWS-CLI. Yes, you heard it right. So why are we doing it anyway?_

**_“Why should all the problems must always have only one solution? I simply like more than one. Be it good or bad”_**

Let’s look at the existing solution first. You install the AWS-CLI and run the following command:

```bash
aws s3 cp s3://my-s3-bucket/ ./ --recursive
```

> **Surprisingly, apart from using the AWS CLI, I didn't find any proper Node.js script or an app that would do this for medium to large scale buckets using the AWS-SDK. The answers I found on Node.js posts online had several problems, including half-baked scripts, scripts that would try and synchronously create a file and would not know when to complete and also would ignore cloning empty folders if there was any. Basically, it didn't do the job right. Hence decided to write one myself, properly.**

I am positive that this would give you a better understanding of how a Node.js application should look and feel, in spite of its size and operation.

> **TLDR;** — Get straight to the code  [**Node-Clone-S3-Bucket**](https://medium.com/r/?url=https%3A%2F%2Fgithub.com%2Frajeshdavidbabu%2FNode-Clone-S3-Bucket "https://github.com/rajeshdavidbabu/Node-Clone-S3-Bucket")[](https://github.com/rajeshdavidbabu/Node-Clone-S3-Bucket)

## Table of Contents

As I said above, I am not gonna explain the code line-by-line, as I am posting the entire base out. Instead, I will talk about how I have architected the application, with a bit of insight into the core-logic and key features. Let me list out what you can expect and get an idea by the end of this write-up.

- **_Project Structure of our Application_**
- **_Core Logic Surrounding the Application_**
- **_Streams in Node.js to Download a File_**
- **_Using AWS-SDK to access S3 APIs_**
- **_Entire Codebase_**

## Project Structure

There is no opinionated approach for building the project structure, it can change from project to project based on the use case. Personally, I split them into smaller independent modules. One module does one type of task and one type of task only.

> Let’s say, if I have a **_network.js_** file**_,_** it does only network operations and doesn’t modify the file structure or create a new file.

Let’s look at our project structure for instance,

![](https://cdn-images-1.medium.com/max/800/1*00Gb_2AuNJuQe_nJ2tK_Nw.png)_Project Structure_

As I said before, there is no particular way to structure your project but its ideal to pick a topic and group all your files under that topic. For me, it was activity, **_“what does that file handle and how ?”._**

Let’s start from the root and go step by step.

### Application Dependencies

These are project dependencies and are essential for development and deployment. And are mostly straight forward to understand:

- _package.json_
- _index.js_
- _git ignore/eslint configs_
- _license, readme etc._
- _node_modules_

And then comes the **_config_** file, the **_config_** file consists of all your application config, api*keys, bucket name, target directory, third party links etc., normally we would have two config files one for production* and one for the _development_ environment.

### Core Entities

Once we made the skeleton of the application ready with the application dependencies, then we have the core entities. In our application, the Core entities include Handler, Service and Storage.

> **_Handler_** is where we glue the entire application and create Services, Storages and inject required dependencies and expose an API for the index.js to handle.

> **_Service_** is where our core-logic of the application lives and all the jobs to other dependencies are delegated from here.

> **_Storage_** is where all our storage operations take place. In our application S3 is the external storage from where we retrieve our files and data from, hence the AWS-SDK operations exclusively happen only inside this storage module.

### Helpers and Utils

When the service starts to run it needs to do all the intended tasks at the same time. For example, in our application, once we get the list of contents under a directory, we need to start creating/cloning the contents locally. This operation is delegated to **_cloner.js_**, a helper which is only responsible for cloning the files and folders. The cloner, in turn, needs to access the **_fileOps.js_** module to create directories and files.

> Under helpers, we have the following files doing their respective chores with or without the help of other helpers.

> **_Cloner_**, handles the cloning of the files and directories with the help of **_fileOps_** module.

> **_Data Handler_**, maps and parses the data from S3 bucket into a usable or consumable data by the service.

> **_Downloader_**, only downloads files from S3 bucket and pipes the operation to a write-stream or simply put, takes care of downloading files asynchronously.

> **_fileOps_**, as the name suggests, uses Node’s **fs** module to create file-streams and directories.

> **_filePath_** provides the entire application with the target folder for cloning the S3 bucket’s files and directories and also returns the target bucket and target root directory on the S3.

> **_Logger_** _inside utils returns an instance of Bunyan logger which can be used to send logs to third parties like Kibana._

## Core Logic Surrounding the Application

Now that we have done our project setup, let’s look into the core logic of the **_service_** module. It involves the sequence of the following actions:

- **_Fetch the list of Keys from the bucket and the target Prefix._** _(check AWS-SDK Javascript APIs)_
- **_Separate the files and directories, because we clone the directories and download the files._**
- **_Clone all the directories first, and then move on to download the files._**
- **_Download the files through streams and log success and failure respectively._** _(AWS-SDK ListKeys API response sometimes ignores giving out directory Keys, hence we need to check for if a directory exists, if not present we create one before downloading the contained file)_

> Additionally, the service not only clones the entire bucket but also clones only specific folders, without losing the folder tree structure, based on our Prefix configuration as specified [here](https://medium.com/r/?url=https%3A%2F%2Fgithub.com%2Frajeshdavidbabu%2FNode-Clone-S3-Bucket%23configuration) as the rootDirectory for cloning.

## Downloading Files Using Streams

Another important concept around the Node.js is using streams to upload and retrieve data from an external source. In our project, the external source is the AWS S3.

> When downloading the files we create a read stream from the AWS SDK getObject method and pipe it to a writeStream which will close automatically once all the data is written. The advantage of using streams is the memory consumption, where the application doesn’t care about the size of the file downloaded.

Our code inside **_storage_** module as shown below uses streams to asynchronously download the data without blocking the event loop.

![](https://cdn-images-1.medium.com/max/800/1*LiO9-odlUOi1HgG-wcSd6w.png)_Node.js streams with AWS getObject_

To dig deeper into Node.js streams, please refer to this write up [here](https://medium.com/r/?url=https%3A%2F%2Fmedium.freecodecamp.org%2Fnode-js-streams-everything-you-need-to-know-c9141306be93).

## Using AWS SDK to access S3

This is the most straight forward topic in the whole application, where you install the **_AWS-SDK_** and start accessing the methods in it. Taking a look at the **_storage_** file would give you a better understanding of how to import and call methods on the same.

## The Codebase of the Application.

Here you can find the entire code for this application, more than reading this, hands-on would give a great deal of information and help you understand the core concepts of this application. Feel free to fork it, play with it and if you like it leave a star on the repo.

[**Node-Clone-S3-Bucket**](https://medium.com/r/?url=https%3A%2F%2Fgithub.com%2Frajeshdavidbabu%2FNode-Clone-S3-Bucket "https://github.com/rajeshdavidbabu/Node-Clone-S3-Bucket")[](https://github.com/rajeshdavidbabu/Node-Clone-S3-Bucket)

## Conclusion

This marks the end of this write-up, hope it gave a better understanding of how to plan, build and run a Node.js service in real-time on a platform such as AWS.
